:author: Ryszard Styczynski
:revnumber: 0.3
:revremark: DRAFT
:revdate: 2025-09-25

:toc: macro
:toc-title: 
:toclevels: 4

= Terrateam Ansible Engine
{author}, v{revnumber} {revremark}, {revdate}

This project integrates Ansible automation into the Terrateam infrastructure-as-code workflow engine. The primary goal is to enable teams to manage both Terraform and Ansible operations seamlessly within Terrateam, supporting layered and multi-stage workflows across different environments and workspaces.

Preliminary capabilities include:

* Support for Ansible playbooks, ansible.cfg, inventory, and galaxy install to enable use of roles and collections
* Galaxy firewall to remove public sources from requirements.yml
* Ansible play creates static execution context e.g. inventory.yml 
* Ansible apply uses static context discovered during the plan phase
* Pipeline control file to control, and debug Ansible execution context

Moreover inheriting capabilities from Terrateam:

* Layered run to handover Terraform to Ansible layer
* GitHub pull request based approval and deployment process
* etc.

This setup is designed to help teams automate complex infrastructure and configuration management tasks, leveraging both Terraform and Ansible in a unified, auditable pipeline.

== Contents
toc::[]

<<<
== Examples

* *day-2_ops1*: Demonstrates a basic Ansible playbook that performs a series of simple tasks on a localhost host, reading parameters from a variable file.

* *day-2_ops2*: Demonstrates gathering collections from a git source using requirements files. Moreover presents ansible_piepline.yml file to control Ansible execution context.

* *day-2_ops3*: Demonstrates use of inventory.ini to provide host names and play variables.

* *day-2_ops4*: Demonstrates use of ansible.cfg to control Ansible execution context.

* *day-2_ops5*: Demonstrates use of galaxy firewall to remove public sources from requirements.yml. Moreover presents ansible_piepline.yml file to control Ansible execution context - here the playbook name is defined to select one out of multiple playbooks.

* *terraform/day-1_cfg*: Contains the Terraform configuration for initial (Day 1) infrastructure provisioning, such as creating VMs, networks, or storage, which serves as the foundation for subsequent Ansible-driven Day 2 operations.

* *day-2_ops_workspace1*: Demonstrates use of workspaces to run Ansible playbooks and change of the engine type. ANSIBLE_ROOT is set to the workspace root. DEV workspace uses runs the debug engine. PROD workspace runs the Ansible engine.

== Ansible Engine capabilities

=== ansible.cfg file detection

An ansible.cfg configuration file placed in the playbook directory will be used, as it is one of the regular locations Ansible checks. The operator may use ansible.cfg to configure the playbook's execution context.

=== Galaxy firewall

Pipeline is extended by a galaxy_firewall.py script to remove public sources from requirements.yml. In case of public sources detection, the requirements.yml is cleared out of forbidden sources, and the user is informed about it in the plan.

```yaml
---
collections:
  - name: collections/ansible_collections/myorg/publicapi/
    type: git
    source: https://github.com/rstyczynski/ansible-collection-howto.git#/collections/ansible_collections/myorg/publicapi
    version: 0.1.2
  # BLOCKED by galaxy_firewall: name: oracle.oci
  # BLOCKED by galaxy_firewall: type: galaxy
  # BLOCKED by galaxy_firewall: source: https://github.com/oracle/ansible-oci-modules.git
  # BLOCKED by galaxy_firewall: version: v2.25.0
roles:
  []
```

=== Galaxy install

The workflow expects a requirements.yml file to be present in the playbook’s root directory, to install dependencies using the standard ansible-galaxy install process. Collections are installed to the regular system directory, which may be changed using ansible.cfg.

[source,bash]
----
ansible-galaxy install -r requirements.yml
----

=== Inventory support

Ansible supports two types of inventory: static and dynamic, generated by plugins (e.g., the OCI collection plugin discovering OCI resources). The Ansible engine supports both, expecting an inventory.ini or inventory.yml file in the playbook’s directory, but always converts them into static YAML format. Having this plugin-based inventory created, during the plan phase ensures it will always be passed to the apply phase in the same form, even when external conditions change (e.g., new machines with given tags are added to the system).

The conversion to a static form is performed using the standard Ansible method.

[source,bash]
----
ansible-inventory -i inventory.yml --list --export --yaml --output inventory_static.yml
----

=== Playbook execution

The plan phase detects the playbook to be executed. When more than one file is found, the plan looks into the ANSIBLE_PLAYBOOK file for a playbook filename. Once the name is determined, the apply phase executes the playbook from its directory with the static inventory file, and captures stderr to a separate file.

Variable files are not applied via the CLI—the playbook should load variable files as required.

[source,bash]
----
ansible-playbook $PLAYBOOK -i inventory_static.yml 2> >(tee /tmp/ansible_stderr.log >&2)
----

=== Pipeline control 

Pipeline detects ansible_piepline.yml file in the playbook directory. The file is used to control the execution context of the Ansible engine. On this stage debug flags, and playbook name are defined.

```yaml
---
ansible_piepline:
  ansible_playbook: duck_ledzeppelin.yml
  debug:
    init: true
    plan: false
    diff: false
    apply: false
    output: false
    shared: false
```

== Ansible and Terrateam Life Cycle

Terrateam implements a Terraform-style lifecycle based on *init → plan → apply → output*, and applies the same model to Ansible. This approach aligns well with enterprise environments where execution requires plan approval, making Ansible workflows auditable and predictable in the same way as Terraform.

Ansible init detects the requirements.yml file and installs defined dependencies using ansible-galaxy. Plan executes ansible-inventory to transform potentially dynamic data generated by plugins into a static file. This step ensures the approver sees exactly what will be executed.

Finally, apply executes ansible-playbook in the context presented in plan. Output writes specified facts into output storage.

The workflow creates a native Ansible execution environment, allowing the operator to run the playbook with the full context of settings from the CLI to ensure that exactly the same will be executed by the pipeline.

== Ansible engine definition

Ansible Engine is defined as series of scripts associated to terrateam stages in `.terrateam/config.yml`.

[source,yaml]
----
  - tag_query: ANS_code
    engine:
      name: custom
      init:    ['${TERRATEAM_ROOT}/.terrateam/ansible/init.sh']
      plan:    ['${TERRATEAM_ROOT}/.terrateam/ansible/plan.sh', '$TERRATEAM_PLAN_FILE']
      diff:    ['${TERRATEAM_ROOT}/.terrateam/ansible/diff.sh', '$TERRATEAM_PLAN_FILE']
      apply:   ['${TERRATEAM_ROOT}/.terrateam/ansible/apply.sh']
      outputs: ['${TERRATEAM_ROOT}/.terrateam/ansible/outputs.sh']
    plan:
      - type: init
      - type: plan
    apply:
      - type: init
      - type: apply
----

Note that init is executed before both plan and apply, as Terrateam runs them in separate execution environments.

_init.sh_ - builds ANSIBLE_ROOT, applied galaxy-firewall to requirements.yml and executes ansible-galaxy install.

_plan.sh_ - discovers the Ansible execution context to document it in a plan file. The plan file is handled by Terrateam to be passed to the apply phase. Note that in this place, potentially dynamic inventory is converted to static form.

_diff.sh_ - converts the plan file to a presentable format for the Pull Request conversation.

_apply.sh_ - unloads the plan to the Ansible directory and executes
ansible-playbook. In reality, only the inventory is unloaded, as the rest of the context is carried by the GitHub repository, and the requirements.yml is processed by t he init script.

_output.sh_ - [Not yet implemented] Writes Ansible facts to a well-known
location.

== To be done

* Discover Ansible neighbors
* Get Terraform properties
* Get Terraform outputs
* Ansible output persistence
